import os
import asyncio

# --- æ ¸å¿ƒï¼šå¯¼å…¥å®˜æ–¹åº“ ---
from langchain_mcp_adapters.client import MultiServerMCPClient

# LangChain/LangGraph ç»„ä»¶
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode

# å¤ç”¨ä½ çš„æµå¼è¾“å‡ºæ¨¡å—å’Œé…ç½®
from m11_mcp_advanced.s01_agent_stream import run_agent_with_streaming
from config import OPENAI_API_KEY, AMAP_MAPS_API_KEY

# === é…ç½® MCP æœåŠ¡å™¨ ===
MCP_SERVERS = {
    # æ–¹å¼1.1: äº‘ç«¯ä»£ç† â€”â€” stdioæ¨¡å¼
    "é«˜å¾·åœ°å›¾": {
        "transport": "stdio",
        "command": "npx",
        "args": ["-y", "@amap/amap-maps-mcp-server"],
        "env": {**os.environ, "AMAP_MAPS_API_KEY": AMAP_MAPS_API_KEY}
    },

    # æ–¹å¼1.2: äº‘ç«¯MCPæœåŠ¡ â€”â€” Streamable HTTPæ¨¡å¼
    # "é«˜å¾·åœ°å›¾" :{
    #     "transport":"streamable_http",
    #     "url": f"https://mcp.amap.com/mcp?key={AMAP_MAPS_API_KEY}"
    # },

    # æ–¹å¼2.1: æœ¬åœ°å·¥å…· â€”â€” stdioæ¨¡å¼
    # "æœ¬åœ°å¤©æ°”":{
    #     "transport": "stdio",
    #     "command": "python",
    #     "args": ["-m", "m10_mcp_basics.s01_stdio_server"],
    #     "env": None
    # },

    # æ–¹å¼2.2:æœ¬åœ°MCPæœåŠ¡ â€”â€” Streamable HTTP æ¨¡å¼
    # æ³¨:æ­¤æ–¹æ³•éœ€è¦æå‰è¿è¡Œm10çš„ s02_streamable_http_server.py
    # "æœ¬åœ°å¤©æ°”":{
    #     "transport":"streamable_http",
    #     "url": "http://127.0.0.1:8001/mcp"
    # }
}


def build_graph(available_tools):
    """æ„å»ºå›¾é€»è¾‘ (ä¿æŒä¸å˜)"""
    if not available_tools:
        print("âš ï¸ æœªåŠ è½½ä»»ä½•å·¥å…·")

    llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=OPENAI_API_KEY,
        base_url="https://api.deepseek.com",
        streaming=True
    )

    llm_with_tools = llm.bind_tools(available_tools) if available_tools else llm

    sys_prompt = "ä½ æ˜¯ä¸€ä¸ªåœ°ç†ä½ç½®åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·éœ€æ±‚è°ƒç”¨å·¥å…·æŸ¥è¯¢ä¿¡æ¯ã€‚"

    async def agent_node(state: MessagesState):
        messages = [SystemMessage(content=sys_prompt)] + state["messages"]
        return {"messages": [await llm_with_tools.ainvoke(messages)]}

    workflow = StateGraph(MessagesState)
    workflow.add_node("agent", agent_node)

    if available_tools:
        workflow.add_node("tools", ToolNode(available_tools))

        def should_continue(state):
            last_msg = state["messages"][-1]
            return "tools" if last_msg.tool_calls else END

        workflow.add_edge(START, "agent")
        workflow.add_conditional_edges("agent", should_continue)
        workflow.add_edge("tools", "agent")
    else:
        workflow.add_edge(START, "agent")
        workflow.add_edge("agent", END)

    return workflow.compile()


async def main():
    print("ğŸ”Œ æ­£åœ¨åˆå§‹åŒ– MCP å®¢æˆ·ç«¯...")

    client = MultiServerMCPClient(MCP_SERVERS)

    # æ˜¾å¼å»ºç«‹è¿æ¥å¹¶è·å–å·¥å…·
    # æ³¨æ„ï¼šè¿™ä¸ª client å¯¹è±¡ä¼šä¿æŒè¿æ¥ï¼Œç›´åˆ°è„šæœ¬ç»“æŸ
    tools = await client.get_tools()
    print(f"âœ… æˆåŠŸåŠ è½½å·¥å…·: {[t.name for t in tools]}")

    # æ„å»ºå¹¶è¿è¡Œ
    app = build_graph(tools)
    query = "å¸®æˆ‘æŸ¥ä¸€ä¸‹æ­å·è¥¿æ¹–é™„è¿‘çš„é…’åº—"
    await run_agent_with_streaming(app, query)


if __name__ == "__main__":
    asyncio.run(main())